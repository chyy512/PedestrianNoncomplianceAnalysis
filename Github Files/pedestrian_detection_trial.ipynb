{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f97841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select 4 points for each zone (C1–C4)\n",
      "Tracking CSV saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial28.csv\n",
      "Video with detections saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial28output.mp4\n"
     ]
    }
   ],
   "source": [
    "#Pedestrian Identification (this was a trial - REID is Terrible for pedestrian tracking)\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "VIDEO_PATH = '/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial2.mp4'\n",
    "OUTPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial28.csv\"\n",
    "OUTPUT_VIDEO = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial28output.mp4\"\n",
    "FRAME_LINE = 10  # frame to use for picking zone points\n",
    "\n",
    "# Zones\n",
    "ZONES = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
    "ZONE_COLORS = {\"C1\": (255,0,0), \"C2\": (0,255,0), \"C3\": (0,0,255), \"C4\": (255,255,0)}\n",
    "\n",
    "# YOLO pedestrian class\n",
    "PEDESTRIAN_CLASS = [0]\n",
    "MODEL_PATH = \"yolo11m-seg.pt\"\n",
    "WRITE_VIDEO = True\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def draw_transparent_polygon(frame, points, color=(0, 0, 255), alpha=0.25):\n",
    "    overlay = frame.copy()\n",
    "    pts = np.array(points, dtype=np.int32)\n",
    "    cv2.fillPoly(overlay, [pts], color)\n",
    "    return cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "def select_zone_points(frame, zone_name, existing_zones):\n",
    "    pts = []\n",
    "    fc = frame.copy()\n",
    "    for zname, zpts in existing_zones.items():\n",
    "        fc = draw_transparent_polygon(fc, zpts, color=ZONE_COLORS.get(zname, (255,0,0)), alpha=0.2)\n",
    "\n",
    "    win = f\"Select {zone_name} - click 4 points (press q to abort)\"\n",
    "    cv2.namedWindow(win, cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(win, fc)\n",
    "\n",
    "    def click(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            pts.append((x, y))\n",
    "            cv2.circle(fc, (x, y), 5, (0, 255, 0), -1)\n",
    "            cv2.imshow(win, fc)\n",
    "            if len(pts) == 4:\n",
    "                cv2.destroyWindow(win)\n",
    "\n",
    "    cv2.setMouseCallback(win, click)\n",
    "    while len(pts) < 4:\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyWindow(win)\n",
    "    return np.array(pts, dtype=np.float32)\n",
    "\n",
    "def point_in_polygon(point, polygon):\n",
    "    poly = np.array(polygon, dtype=np.int32)\n",
    "    return cv2.pointPolygonTest(poly, (int(point[0]), int(point[1])), False) >= 0\n",
    "\n",
    "def get_zone_from_point(point, zones_src):\n",
    "    for zone_name, pts in zones_src.items():\n",
    "        if point_in_polygon(point, pts):\n",
    "            return zone_name\n",
    "    return None\n",
    "\n",
    "def frame_to_hms(frame_num, fps):\n",
    "    seconds = frame_num / fps\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{h:01d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "last_positions = {}  # PersonID → (x, y)\n",
    "\n",
    "# def get_direction(prev, curr, threshold=5):\n",
    "#     dx = curr[0] - prev[0]  # horizontal movement\n",
    "\n",
    "#     # Ignore very small horizontal movements (jitter)\n",
    "#     if abs(dx) < threshold:\n",
    "#         return None\n",
    "\n",
    "#     # Decide horizontal direction\n",
    "#     if dx > 0:\n",
    "#         return \"Right\"  # moving towards right of screen\n",
    "#     else:\n",
    "#         return \"Left\"   # moving towards left of screen\n",
    "    \n",
    "\n",
    "# ===================== VIDEO & ZONE SELECTION =====================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, FRAME_LINE)\n",
    "ret, frame_line = cap.read()\n",
    "if not ret:\n",
    "    cap.release()\n",
    "    raise RuntimeError(f\"Cannot read frame {FRAME_LINE}\")\n",
    "\n",
    "ZONE_SRC_PTS = {}\n",
    "print(\"Select 4 points for each zone (C1–C4)\")\n",
    "for zone in ZONES:\n",
    "    src_pts = select_zone_points(frame_line, zone, ZONE_SRC_PTS)\n",
    "    if src_pts.shape[0] != 4:\n",
    "        raise RuntimeError(f\"Zone {zone} selection aborted or not enough points.\")\n",
    "    ZONE_SRC_PTS[zone] = src_pts\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Zone persistence settings\n",
    "last_zones = {}       # personID → last known zone\n",
    "outside_count = {}    # personID → consecutive frames outside\n",
    "OUTSIDE_GRACE = 30     # allow this many frames of \"None\" before clearing zone\n",
    "\n",
    "# ===================== INITIALIZE YOLO =====================\n",
    "model = YOLO(MODEL_PATH)\n",
    "\n",
    "frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "if WRITE_VIDEO:\n",
    "    cap.release()\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    vid_out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (frame_w, frame_h))\n",
    "\n",
    "# ===================== TRACKING & CSV LOGGING =====================\n",
    "all_rows = []\n",
    "frame_num = 0\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_num += 1\n",
    "        timestamp_text = frame_to_hms(frame_num, fps)\n",
    "\n",
    "        results = model.track(frame, persist=True, classes=PEDESTRIAN_CLASS, verbose=False, tracker = '/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/bytetrack_test.yaml')\n",
    "\n",
    "        # Draw zones\n",
    "        for zname, pts in ZONE_SRC_PTS.items():\n",
    "            frame = draw_transparent_polygon(frame, pts, color=ZONE_COLORS.get(zname, (200,200,200)), alpha=0.12)\n",
    "\n",
    "        if results and len(results) > 0 and results[0].boxes is not None:\n",
    "            boxes = results[0].boxes\n",
    "            try:\n",
    "                ids = boxes.id.int().cpu().tolist() if boxes.id is not None else [None]*len(boxes)\n",
    "            except:\n",
    "                ids = [None]*len(boxes)\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            try:\n",
    "                classes = boxes.cls.int().cpu().tolist()\n",
    "            except:\n",
    "                classes = [None]*len(boxes)\n",
    "\n",
    "            for idx, box in enumerate(xyxy):\n",
    "                pid = ids[idx] or f\"det_{frame_num}_{idx}\"\n",
    "                cls = classes[idx] if idx < len(classes) else None\n",
    "                if cls not in PEDESTRIAN_CLASS:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = box\n",
    "                cx, cy = int((x1 + x2)/2), int(y2)\n",
    "                pixel_pos = (cx, cy)\n",
    "\n",
    "                # Determine zone with persistence\n",
    "                zone = get_zone_from_point(pixel_pos, ZONE_SRC_PTS)\n",
    "\n",
    "                if zone is None and pid in last_zones:\n",
    "                    # If recently inside a zone, keep it until grace expires\n",
    "                    if outside_count.get(pid, 0) < OUTSIDE_GRACE:\n",
    "                        zone = last_zones[pid]\n",
    "                        outside_count[pid] = outside_count.get(pid, 0) + 1\n",
    "                    else:\n",
    "                        zone = None\n",
    "                else:\n",
    "                    outside_count[pid] = 0  # reset if detected inside\n",
    "\n",
    "                if zone is not None:\n",
    "                    last_zones[pid] = zone\n",
    "                \n",
    "                # direction = None\n",
    "                # if pid in last_positions:\n",
    "                #     prev_pos = last_positions[pid]\n",
    "                #     direction = get_direction(prev_pos, pixel_pos)\n",
    "\n",
    "                # last_positions[pid] = pixel_pos  # update for next frame\n",
    "\n",
    "                # Append row (only once, with direction)\n",
    "                all_rows.append([timestamp_text, pid, frame_num, cx, cy, zone])\n",
    "\n",
    "                # Draw detection + ID\n",
    "                cv2.circle(frame, pixel_pos, 4, (0, 255, 0), -1)\n",
    "                cv2.putText(frame, f\"{pid}\", (cx, max(15, cy-10)), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0,255,0),1)\n",
    "                # cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255,0,0), 2)\n",
    "                # Draw timestamp on video\n",
    "                cv2.putText(frame, f\"Time: {timestamp_text}\", (10, frame_h - 15),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        if WRITE_VIDEO:\n",
    "            vid_out.write(frame)\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    if WRITE_VIDEO:\n",
    "        vid_out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ===================== SAVE CSV =====================\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Timestamp\", \"PersonID\", \"Frame\", \"PixelX\", \"PixelY\", \"Zone\"])\n",
    "    writer.writerows(all_rows)\n",
    "\n",
    "print(f\"Tracking CSV saved to {OUTPUT_CSV}\")\n",
    "print(f\"Video with detections saved to {OUTPUT_VIDEO}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fea020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication removed 262730 rows.\n",
      "Deduplication removed 280626 rows in total (including flickers).\n",
      "Mapping of non-numeric IDs:\n",
      "              OldID  NewID\n",
      "0        det_1848_0   3491\n",
      "1        det_1851_0   3492\n",
      "2        det_1853_0   3493\n",
      "3        det_1855_0   3494\n",
      "4        det_1856_0   3495\n",
      "...             ...    ...\n",
      "17879  det_225974_1  21370\n",
      "17880  det_225974_2  21371\n",
      "17881  det_225974_3  21372\n",
      "17882  det_225975_0  21373\n",
      "17883  det_225975_1  21374\n",
      "\n",
      "[17884 rows x 2 columns]\n",
      "CSV with arrival info and deduplication saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone.csv\n",
      "  Timestamp  PersonID  Frame  PixelX  PixelY Zone ArrivalTime ArrivalZone\n",
      "0   0:00:00         1      1     993     402   C2     0:00:00          C2\n",
      "1   0:00:01         1     92     992     402   C2     0:00:00          C2\n",
      "2   0:00:01         1     93     993     402   C2     0:00:00          C2\n",
      "3   0:00:01         1    106     992     402   C2     0:00:00          C2\n",
      "4   0:00:01         1    118     993     402   C2     0:00:00          C2\n",
      "5   0:00:02         1    121     992     402   C2     0:00:00          C2\n",
      "6   0:00:02         1    127     993     402   C2     0:00:00          C2\n",
      "7   0:00:02         1    132     992     402   C2     0:00:00          C2\n",
      "8   0:00:02         1    133     993     402   C2     0:00:00          C2\n",
      "9   0:00:02         1    134     992     402   C2     0:00:00          C2\n"
     ]
    }
   ],
   "source": [
    "#arrival time and arrival zone per ID - good\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM.csv\"\n",
    "OUTPUT_CSV = INPUT_CSV.replace(\".csv\", \"-arrivaltimezone.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# ===================== CONVERT NUMERIC IDS =====================\n",
    "# Attempt to convert numeric IDs; keep others as NaN temporarily\n",
    "df[\"PersonID_numeric\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\")\n",
    "\n",
    "# Find the maximum existing numeric ID\n",
    "max_id = df[\"PersonID_numeric\"].max()\n",
    "if pd.isna(max_id):\n",
    "    max_id = 0\n",
    "max_id = int(max_id)\n",
    "\n",
    "# Map non-numeric IDs to new numeric IDs\n",
    "new_ids = {}  # store mapping of old -> new numeric ID\n",
    "counter = max_id + 1\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row[\"PersonID_numeric\"]):\n",
    "        old_id = row[\"PersonID\"]\n",
    "        if old_id not in new_ids:\n",
    "            new_ids[old_id] = counter\n",
    "            counter += 1\n",
    "        df.at[i, \"PersonID_numeric\"] = new_ids[old_id]\n",
    "\n",
    "# Replace PersonID with numeric version\n",
    "df[\"PersonID\"] = df[\"PersonID_numeric\"].astype(int)\n",
    "df = df.drop(columns=[\"PersonID_numeric\"])\n",
    "\n",
    "# ===================== ARRIVAL INFO =====================\n",
    "df = df.sort_values([\"PersonID\", \"Frame\"]).reset_index(drop=True)\n",
    "arrival_info = {}\n",
    "# direction_info = {}\n",
    "\n",
    "for pid, group in df.groupby(\"PersonID\"):\n",
    "    first_row = group.iloc[0]\n",
    "    last_row = group.iloc[-1]\n",
    "\n",
    "    # Arrival info\n",
    "    arrival_info[pid] = {\n",
    "        \"ArrivalTime\": first_row[\"Timestamp\"],\n",
    "        \"ArrivalZone\": first_row[\"Zone\"]\n",
    "    }\n",
    "\n",
    "    # # Direction based on first vs last PixelX\n",
    "    # if last_row[\"PixelX\"] > first_row[\"PixelX\"]:\n",
    "    #     direction = \"Right\"\n",
    "    # elif last_row[\"PixelX\"] < first_row[\"PixelX\"]:\n",
    "    #     direction = \"Left\"\n",
    "    # else:\n",
    "    #     direction = \"Stationary\"\n",
    "\n",
    "    # direction_info[pid] = direction ###--------------- move to later passes\n",
    "\n",
    "# Map arrival info back to all rows\n",
    "df[\"ArrivalTime\"] = df[\"PersonID\"].map(lambda x: arrival_info[x][\"ArrivalTime\"])\n",
    "df[\"ArrivalZone\"] = df[\"PersonID\"].map(lambda x: arrival_info[x][\"ArrivalZone\"])\n",
    "# df[\"Direction\"] = df[\"PersonID\"].map(lambda x: direction_info[x])\n",
    "\n",
    "# # ===================== DEPARTURE INFO ===================== add to later passes it works yay\n",
    "# departure_info = {}\n",
    "\n",
    "# for pid, group in df.groupby(\"PersonID\"):\n",
    "#     group = group.sort_values(\"Frame\")\n",
    "#     prev_zone = group.iloc[0][\"Zone\"]\n",
    "#     departure_time = None\n",
    "#     departure_zone = None\n",
    "\n",
    "#     for i in range(1, len(group)):\n",
    "#         current_zone = group.iloc[i][\"Zone\"]\n",
    "#         if current_zone != prev_zone:\n",
    "#             departure_time = group.iloc[i][\"Timestamp\"]\n",
    "#             departure_zone = prev_zone\n",
    "#             break  # stop at the first departure\n",
    "#         prev_zone = current_zone\n",
    "\n",
    "#     departure_info[pid] = {\n",
    "#         \"DepartureTime\": departure_time,\n",
    "#         \"DepartureZone\": departure_zone\n",
    "#     }\n",
    "\n",
    "# df[\"DepartureTime\"] = df[\"PersonID\"].map(lambda x: departure_info[x][\"DepartureTime\"])\n",
    "# df[\"DepartureZone\"] = df[\"PersonID\"].map(lambda x: departure_info[x][\"DepartureZone\"])\n",
    "\n",
    "# ===================== DEDUPLICATE STATIONARY ROWS =====================\n",
    "# Remove consecutive rows for the same PersonID with same PixelX and PixelY\n",
    "df = df.sort_values([\"PersonID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "before_rows = len(df)\n",
    "\n",
    "mask = (df[\"PersonID\"] != df[\"PersonID\"].shift()) | \\\n",
    "       (df[\"PixelX\"] != df[\"PixelX\"].shift()) | \\\n",
    "       (df[\"PixelY\"] != df[\"PixelY\"].shift())\n",
    "\n",
    "df = df[mask].reset_index(drop=True)\n",
    "\n",
    "after_rows = len(df)\n",
    "removed_rows = before_rows - after_rows\n",
    "print(f\"Deduplication removed {removed_rows} rows.\")\n",
    "\n",
    "df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# ===================== REMOVE SINGLE-FRAME (FLICKER) DETECTIONS =====================\n",
    "frame_counts = df.groupby(\"PersonID\")[\"Frame\"].count()\n",
    "valid_pids = frame_counts[frame_counts > 1].index  # only keep PersonIDs with more than 1 row\n",
    "\n",
    "df = df[df[\"PersonID\"].isin(valid_pids)].reset_index(drop=True)\n",
    "\n",
    "after_rows = len(df)\n",
    "removed_rows = before_rows - after_rows\n",
    "print(f\"Deduplication removed {before_rows - after_rows} rows in total (including flickers).\")\n",
    "\n",
    "# Ensure PersonID is integer\n",
    "df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# Save mapping of non-numeric IDs (if any)\n",
    "if new_ids:\n",
    "    mapping_df = pd.DataFrame(list(new_ids.items()), columns=[\"OldID\", \"NewID\"])\n",
    "    print(\"Mapping of non-numeric IDs:\")\n",
    "    print(mapping_df)\n",
    "\n",
    "print(f\"CSV with arrival info and deduplication saved to {OUTPUT_CSV}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processed merged CSV saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-postprocess-mergedIDs.csv\n"
     ]
    }
   ],
   "source": [
    "# #post porcessing unification\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import math\n",
    "\n",
    "# # ===================== CONFIG =====================\n",
    "# INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial23-arrivaltimezone.csv\"\n",
    "# OUTPUT_CSV = INPUT_CSV.replace(\".csv\", \"-firstlastrow.csv\")\n",
    "\n",
    "# # ===================== LOAD =====================\n",
    "# df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# # Sort by PersonID then Frame\n",
    "# df = df.sort_values([\"PersonID\", \"Frame\"]).reset_index(drop=True)\n",
    "# # # Keep a mapping of final merged IDs\n",
    "# # id_mapping = {}\n",
    "\n",
    "# # # Helper function: Euclidean distance\n",
    "# # def distance(p1, p2):\n",
    "# #     return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "# # # =====================MERGE PROCESS =====================\n",
    "# # # Store last known position of each merged ID\n",
    "# # last_positions = {}\n",
    "\n",
    "# # for pid, group in df.groupby(\"PersonID\"):\n",
    "# #     first_row = group.iloc[0]\n",
    "# #     last_row = group.iloc[-1]\n",
    "\n",
    "# #     new_id = pid\n",
    "# #     first_pos = (first_row[\"PixelX\"], first_row[\"PixelY\"])\n",
    "# #     first_frame = first_row[\"Frame\"]\n",
    "\n",
    "# #     best_match = None\n",
    "# #     best_dist = float(\"inf\")\n",
    "\n",
    "# #     # Check all existing merged IDs\n",
    "# #     for existing_id, (ex_frame, ex_pos) in last_positions.items():\n",
    "# #         if ex_frame < first_frame:  # only look at earlier tracks\n",
    "# #             d = distance(first_pos, ex_pos)\n",
    "# #             if DIST_MIN <= d <= DIST_MAX and d < best_dist:\n",
    "# #                 best_match = existing_id\n",
    "# #                 best_dist = d\n",
    "\n",
    "# #     if best_match is not None:\n",
    "# #         # Merge: map this PersonID to existing one\n",
    "# #         id_mapping[new_id] = best_match\n",
    "# #         # Update last position of matched ID with this new track’s last row\n",
    "# #         last_positions[best_match] = (last_row[\"Frame\"], (last_row[\"PixelX\"], last_row[\"PixelY\"]))\n",
    "# #     else:\n",
    "# #         # Keep as new identity\n",
    "# #         id_mapping[new_id] = new_id\n",
    "# #         last_positions[new_id] = (last_row[\"Frame\"], (last_row[\"PixelX\"], last_row[\"PixelY\"]))\n",
    "\n",
    "# # # ===================== APPLY MERGE =====================\n",
    "# # df[\"MergedID\"] = df[\"PersonID\"].map(id_mapping)\n",
    "\n",
    "\n",
    "# # ===================== SAVE =====================\n",
    "# os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "# df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# print(f\"Post-processed merged CSV saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdb4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/kfv0kct16s1gdd4k7rzps5tr0000gn/T/ipykernel_76482/2813270566.py:17: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_CSV, header=None, names=[\"Time\", \"PersonID\", \"Frame\", \"X\", \"Y\", \"Zone\", \"Direction\", \"ArrivalTime\", \"ArrivalZone\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge1.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge1summary.csv\n",
      "       Time PersonID  Frame      X      Y Zone Direction ArrivalTime  \\\n",
      "1   0:00:00        1    1.0  993.0  402.0   C2   0:00:00          C2   \n",
      "2   0:00:01        1   92.0  992.0  402.0   C2   0:00:00          C2   \n",
      "3   0:00:01        1   93.0  993.0  402.0   C2   0:00:00          C2   \n",
      "4   0:00:01        1  106.0  992.0  402.0   C2   0:00:00          C2   \n",
      "5   0:00:01        1  118.0  993.0  402.0   C2   0:00:00          C2   \n",
      "6   0:00:02        1  121.0  992.0  402.0   C2   0:00:00          C2   \n",
      "7   0:00:02        1  127.0  993.0  402.0   C2   0:00:00          C2   \n",
      "8   0:00:02        1  132.0  992.0  402.0   C2   0:00:00          C2   \n",
      "9   0:00:02        1  133.0  993.0  402.0   C2   0:00:00          C2   \n",
      "10  0:00:02        1  134.0  992.0  402.0   C2   0:00:00          C2   \n",
      "\n",
      "    ArrivalZone  MergeID  \n",
      "1           NaN        1  \n",
      "2           NaN        1  \n",
      "3           NaN        1  \n",
      "4           NaN        1  \n",
      "5           NaN        1  \n",
      "6           NaN        1  \n",
      "7           NaN        1  \n",
      "8           NaN        1  \n",
      "9           NaN        1  \n",
      "10          NaN        1  \n"
     ]
    }
   ],
   "source": [
    "#pass 1 : merge 1 \n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone.csv\"\n",
    "FPS = 30  # frames per second (needed to convert seconds to frames)\n",
    "X_THRESH = 10  # pixels\n",
    "Y_THRESH = 5  # pixels\n",
    "TIME_THRESH_SEC = 2  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\".csv\", \"-merge1.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\".csv\", \"-merge1summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV, header=None, names=[\"Time\", \"PersonID\", \"Frame\", \"X\", \"Y\", \"Zone\", \"Direction\", \"ArrivalTime\", \"ArrivalZone\"])\n",
    "\n",
    "# Convert numeric columns\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"PersonID\"] = df[\"PersonID\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"PersonID\", \"Frame\", \"X\", \"Y\"]) # there shouldn't be any\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"PersonID\").first().reset_index() # Get the first observation (row) for each PersonID\n",
    "last_obs = df.groupby(\"PersonID\").last().reset_index() # Get the last observation (row) for each PersonID\n",
    "\n",
    "# Merge first and last for easier comparison \n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"PersonID\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each PersonID is a node\n",
    "for pid in first_last_df[\"PersonID\"]:\n",
    "    G.add_node(pid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.PersonID == row_b.PersonID:\n",
    "            continue\n",
    "\n",
    "        # Spatial proximity: last of A to first of B\n",
    "        dx = abs(row_a.X_last - row_b.X_first)\n",
    "        dy = abs(row_a.Y_last - row_b.Y_first)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            # Link these two IDs\n",
    "            G.add_edge(row_a.PersonID, row_b.PersonID)\n",
    "\n",
    "# ===================== ASSIGN MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for pid in comp:\n",
    "        merge_map[pid] = merge_id\n",
    "\n",
    "# Assign MergeID back to full dataset\n",
    "df[\"MergeID\"] = df[\"PersonID\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "# df[\"MergeID\"] = pd.to_numeric(df[\"MergeID\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# #sort by MergeID, PersonID, then Frame\n",
    "# df = df.sort_values([\"MergeID\", \"PersonID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "# # Reorder columns so MergeID is first\n",
    "# cols = [\"MergeID\"] + [c for c in df.columns if c != \"MergeID\"]\n",
    "# df = df[cols]\n",
    "\n",
    "# # ===================== SORT DATA FOR FINAL OUTPUT =====================\n",
    "# # Sort by MergeID, then PersonID, then Frame\n",
    "# df = df.sort_values(\n",
    "#     by=[\"MergeID\", \"PersonID\", \"Frame\"]\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID\"].unique()):\n",
    "    subset = df[df[\"MergeID\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID\": merge_id,\n",
    "        \"PersonIDs\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4685f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 2 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge2.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge2summary.csv\n",
      "      Time  PersonID  Frame      X      Y Zone Direction ArrivalTime  \\\n",
      "0  0:00:00         1    1.0  993.0  402.0   C2   0:00:00          C2   \n",
      "1  0:00:01         1   92.0  992.0  402.0   C2   0:00:00          C2   \n",
      "2  0:00:01         1   93.0  993.0  402.0   C2   0:00:00          C2   \n",
      "3  0:00:01         1  106.0  992.0  402.0   C2   0:00:00          C2   \n",
      "4  0:00:01         1  118.0  993.0  402.0   C2   0:00:00          C2   \n",
      "5  0:00:02         1  121.0  992.0  402.0   C2   0:00:00          C2   \n",
      "6  0:00:02         1  127.0  993.0  402.0   C2   0:00:00          C2   \n",
      "7  0:00:02         1  132.0  992.0  402.0   C2   0:00:00          C2   \n",
      "8  0:00:02         1  133.0  993.0  402.0   C2   0:00:00          C2   \n",
      "9  0:00:02         1  134.0  992.0  402.0   C2   0:00:00          C2   \n",
      "\n",
      "   ArrivalZone MergeID  MergeID2  \n",
      "0          NaN       1         1  \n",
      "1          NaN       1         1  \n",
      "2          NaN       1         1  \n",
      "3          NaN       1         1  \n",
      "4          NaN       1         1  \n",
      "5          NaN       1         1  \n",
      "6          NaN       1         1  \n",
      "7          NaN       1         1  \n",
      "8          NaN       1         1  \n",
      "9          NaN       1         1  \n"
     ]
    }
   ],
   "source": [
    "#passs 2\n",
    "# ===================== PASS 2 MERGE (Bounding Box Method) =====================\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge1.csv\"\n",
    "FPS = 30  # frames per second\n",
    "X_THRESH = 15  # max difference in X (pixels)\n",
    "Y_THRESH = 10  # max difference in Y (pixels)\n",
    "TIME_THRESH_SEC = 2  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"1.csv\", \"2.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"1.csv\", \"2summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID\"] = df[\"MergeID\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"MergeID\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID is a node\n",
    "for mid in first_last_df[\"MergeID\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID == row_b.MergeID:\n",
    "            continue\n",
    "\n",
    "        # Bounding box proximity: last of A to first of B\n",
    "        dx = abs(row_a.X_last - row_b.X_first)\n",
    "        dy = abs(row_a.Y_last - row_b.Y_first)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID, row_b.MergeID)\n",
    "\n",
    "# ===================== ASSIGN PASS 2 MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for mid in comp:\n",
    "        merge_map[mid] = merge_id\n",
    "\n",
    "# Map new MergeID back to full dataset\n",
    "df[\"MergeID2\"] = df[\"MergeID\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "# df[\"MergeID2\"] = pd.to_numeric(df[\"MergeID2\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Sort and reorder columns\n",
    "df = df.sort_values([\"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "# cols = [\"MergeID2\"] + [c for c in df.columns if c != \"MergeID2\"]\n",
    "# df = df[cols]\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID2 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID2\"].unique()):\n",
    "    subset = df[df[\"MergeID2\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID2\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 2 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6dfa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 3 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge3.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge3summary.csv\n",
      "      Time  PersonID  Frame      X      Y Zone Direction ArrivalTime  \\\n",
      "0  0:00:00         1    1.0  993.0  402.0   C2   0:00:00          C2   \n",
      "1  0:00:01         1   92.0  992.0  402.0   C2   0:00:00          C2   \n",
      "2  0:00:01         1   93.0  993.0  402.0   C2   0:00:00          C2   \n",
      "3  0:00:01         1  106.0  992.0  402.0   C2   0:00:00          C2   \n",
      "4  0:00:01         1  118.0  993.0  402.0   C2   0:00:00          C2   \n",
      "5  0:00:02         1  121.0  992.0  402.0   C2   0:00:00          C2   \n",
      "6  0:00:02         1  127.0  993.0  402.0   C2   0:00:00          C2   \n",
      "7  0:00:02         1  132.0  992.0  402.0   C2   0:00:00          C2   \n",
      "8  0:00:02         1  133.0  993.0  402.0   C2   0:00:00          C2   \n",
      "9  0:00:02         1  134.0  992.0  402.0   C2   0:00:00          C2   \n",
      "\n",
      "   ArrivalZone MergeID MergeID2  MergeID3  \n",
      "0          NaN       1        1         1  \n",
      "1          NaN       1        1         1  \n",
      "2          NaN       1        1         1  \n",
      "3          NaN       1        1         1  \n",
      "4          NaN       1        1         1  \n",
      "5          NaN       1        1         1  \n",
      "6          NaN       1        1         1  \n",
      "7          NaN       1        1         1  \n",
      "8          NaN       1        1         1  \n",
      "9          NaN       1        1         1  \n"
     ]
    }
   ],
   "source": [
    "#pass 3 : merge 3\n",
    "# ===================== PASS 3 MERGE (Bounding Box Method) =====================\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge2.csv\"\n",
    "FPS = 30  # frames per second\n",
    "X_THRESH = 20  # max difference in X (pixels)\n",
    "Y_THRESH = 15  # max difference in Y (pixels)\n",
    "TIME_THRESH_SEC = 2  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"2.csv\", \"3.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"2.csv\", \"3summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID\"] = df[\"MergeID\"].astype(str)\n",
    "df[\"MergeID2\"] = df[\"MergeID2\"].astype(str)\n",
    "\n",
    "\n",
    "df = df.dropna(subset=[\"MergeID2\", \"MergeID\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID2\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID2\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID2\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID2 is a node\n",
    "for mid in first_last_df[\"MergeID2\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID2 == row_b.MergeID2:\n",
    "            continue\n",
    "\n",
    "        # Bounding box proximity: last of A to first of B\n",
    "        dx = abs(row_a.X_last - row_b.X_first)\n",
    "        dy = abs(row_a.Y_last - row_b.Y_first)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID2, row_b.MergeID2)\n",
    "\n",
    "# ===================== ASSIGN PASS 3 MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for mid in comp:\n",
    "        merge_map[mid] = merge_id\n",
    "\n",
    "# Map new MergeID back to full dataset\n",
    "df[\"MergeID3\"] = df[\"MergeID2\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "# df[\"MergeID2\"] = pd.to_numeric(df[\"MergeID2\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Sort and reorder columns\n",
    "df = df.sort_values([\"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "# cols = [\"MergeID2\"] + [c for c in df.columns if c != \"MergeID2\"]\n",
    "# df = df[cols]\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID3 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID3\"].unique()):\n",
    "    subset = df[df[\"MergeID3\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID3\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 3 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415236d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 4 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge4.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge4summary.csv\n",
      "   MergeID4 PersonID FirstTime  FirstFrame  FirstX  FirstY FirstZone LastTime  \\\n",
      "0         1      [1]   0:00:00         1.0   993.0   402.0        C2  0:00:21   \n",
      "1         2   [1266]   0:24:17     87341.0  1006.0   404.0        C2  0:24:17   \n",
      "2         3    [791]   0:17:28     62868.0   828.0   383.0       NaN  0:17:29   \n",
      "3         4   [2660]   0:46:42    167988.0   128.0   407.0       NaN  0:46:42   \n",
      "4         5   [2664]   0:46:44    168100.0    62.0   413.0       NaN  0:46:44   \n",
      "5         6   [2666]   0:46:46    168204.0  1020.0   382.0        C2  0:46:52   \n",
      "6         7   [2667]   0:46:52    168603.0  1170.0   383.0       NaN  0:46:53   \n",
      "7         8   [2668]   0:46:53    168654.0   881.0   383.0       NaN  0:46:53   \n",
      "8         9   [2669]   0:46:54    168717.0   723.0   401.0       NaN  0:46:55   \n",
      "9        10   [2380]   0:42:15    151955.0   816.0   390.0       NaN  0:42:15   \n",
      "\n",
      "   LastFrame   LastX  LastY LastZone  \n",
      "0     1265.0   522.0  493.0       C3  \n",
      "1    87351.0  1009.0  388.0       C2  \n",
      "2    62897.0   844.0  371.0      NaN  \n",
      "3   167995.0   123.0  402.0      NaN  \n",
      "4   168114.0    47.0  414.0      NaN  \n",
      "5   168555.0  1115.0  385.0      NaN  \n",
      "6   168647.0  1221.0  376.0      NaN  \n",
      "7   168665.0   861.0  386.0      NaN  \n",
      "8   168747.0   577.0  413.0      NaN  \n",
      "9   151990.0   798.0  387.0      NaN  \n"
     ]
    }
   ],
   "source": [
    "#pass 4 : merge 4 (Euclidean with min/max thresholds)\n",
    "# ===================== PASS 4 MERGE (Euclidean Distance with Min/Max) =====================\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge3.csv\"\n",
    "FPS = 30  # frames per second\n",
    "DIST_MIN = 0   # minimum Euclidean distance to merge (pixels)\n",
    "DIST_MAX = 10   # maximum Euclidean distance to merge (pixels)\n",
    "TIME_THRESH_SEC = 3  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "TIME_MIN_SEC = 0.3  # minimum time difference in seconds\n",
    "FRAME_MIN = TIME_MIN_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"3.csv\", \"4.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"3.csv\", \"4summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID\"] = df[\"MergeID\"].astype(str)\n",
    "df[\"MergeID2\"] = df[\"MergeID2\"].astype(str)\n",
    "df[\"MergeID3\"] = df[\"MergeID3\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID3\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID3\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID3\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID3 is a node\n",
    "for mid in first_last_df[\"MergeID3\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID3 == row_b.MergeID3:\n",
    "            continue\n",
    "\n",
    "        # Euclidean distance\n",
    "        dist = np.sqrt((row_a.X_last - row_b.X_first)**2 + (row_a.Y_last - row_b.Y_first)**2)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        # Merge only if distance is within min/max AND frame_diff within min/max\n",
    "        if DIST_MIN <= dist <= DIST_MAX and FRAME_MIN <= frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID3, row_b.MergeID3)\n",
    "\n",
    "# ===================== ASSIGN PASS 4 MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for mid in comp:\n",
    "        merge_map[mid] = merge_id\n",
    "\n",
    "# Map new MergeID back to full dataset\n",
    "df[\"MergeID4\"] = df[\"MergeID3\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# Sort and reorder columns\n",
    "df = df.sort_values([\"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID4 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID4\"].unique()):\n",
    "    subset = df[df[\"MergeID4\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID4\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 4 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "# print(df.head(10))\n",
    "print(summary_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a8a1771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 5 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge5.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge5summary.csv\n",
      "   MergeID5            PersonID FirstTime  FirstFrame  FirstX  FirstY  \\\n",
      "0         1                 [1]   0:00:00         1.0   993.0   402.0   \n",
      "1         2        [2380, 2386]   0:42:15    151955.0   816.0   390.0   \n",
      "2         3        [2776, 2809]   0:48:21    173908.0    27.0   621.0   \n",
      "3         4  [1453, 1465, 1469]   0:28:51    103798.0  1060.0   396.0   \n",
      "4         5              [1454]   0:28:52    103827.0    35.0   555.0   \n",
      "5         6        [1455, 1467]   0:28:52    103864.0    80.0   455.0   \n",
      "6         7              [1456]   0:28:53    103892.0   907.0   391.0   \n",
      "7         8              [1037]   0:21:47     78384.0   469.0   512.0   \n",
      "8         9               [146]   0:04:04     14679.0   348.0   382.0   \n",
      "9        10        [1460, 1461]   0:28:54    103973.0   171.0   474.0   \n",
      "\n",
      "  FirstZone LastTime  LastFrame  LastX  LastY LastZone  \n",
      "0        C2  0:00:21     1265.0  522.0  493.0       C3  \n",
      "1       NaN  0:42:17   152110.0  794.0  382.0      NaN  \n",
      "2       NaN  0:48:59   176209.0  344.0  391.0       C4  \n",
      "3       NaN  0:28:59   104238.0  986.0  398.0       C2  \n",
      "4       NaN  0:29:50   107341.0  468.0  465.0       C3  \n",
      "5       NaN  0:29:07   104721.0   14.0  560.0      NaN  \n",
      "6        C2  0:28:53   103923.0  900.0  392.0       C2  \n",
      "7        C3  0:21:58    79058.0   55.0  439.0       C3  \n",
      "8        C4  0:04:05    14704.0  349.0  383.0       C4  \n",
      "9       NaN  0:28:55   104012.0  177.0  465.0      NaN  \n"
     ]
    }
   ],
   "source": [
    "# ===================== PASS 5 MERGE (Direct Euclidean Distance Merge) =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge4.csv\"\n",
    "FPS = 30  # frames per second\n",
    "DIST_MIN = 2   # minimum Euclidean distance to merge (pixels)\n",
    "DIST_MAX = 50   # maximum Euclidean distance to merge (pixels)\n",
    "TIME_THRESH_SEC = 3  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "TIME_MIN_SEC = 0.5  # minimum time difference in seconds\n",
    "FRAME_MIN = TIME_MIN_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"4.csv\", \"5.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"4.csv\", \"5summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID4\"] = df[\"MergeID4\"].astype(str)\n",
    "df = df.dropna(subset=[\"MergeID4\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID4\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID4\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID4\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== DIRECT MERGE LOGIC =====================\n",
    "merge_map = {}\n",
    "next_merge_id = 1\n",
    "used_nodes = set()\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    if row_a.MergeID4 in used_nodes:\n",
    "        continue\n",
    "\n",
    "    # Find valid candidates within thresholds\n",
    "    candidates = []\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID4 == row_b.MergeID4 or row_b.MergeID4 in used_nodes:\n",
    "            continue\n",
    "\n",
    "        dist = np.sqrt((row_a.X_last - row_b.X_first)**2 + (row_a.Y_last - row_b.Y_first)**2)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if DIST_MIN <= dist <= DIST_MAX and FRAME_MIN <= frame_diff <= FRAME_THRESH:\n",
    "            candidates.append((dist, row_b.MergeID4))\n",
    "\n",
    "    # Merge with the closest candidate only (or keep alone)\n",
    "    merge_map[row_a.MergeID4] = next_merge_id\n",
    "    used_nodes.add(row_a.MergeID4)\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort()  # sort by distance\n",
    "        closest = candidates[0][1]\n",
    "        merge_map[closest] = next_merge_id\n",
    "        used_nodes.add(closest)\n",
    "\n",
    "    next_merge_id += 1\n",
    "\n",
    "# ===================== MAP BACK TO FULL DATA =====================\n",
    "df[\"MergeID5\"] = df[\"MergeID4\"].map(merge_map).fillna(-1).astype(int)\n",
    "df = df.sort_values([\"MergeID5\", \"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID5 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID5\"].unique()):\n",
    "    subset = df[df[\"MergeID5\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID5\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 5 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(summary_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51175001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 6 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge6.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge6summary.csv\n",
      "   MergeID6                                           PersonID FirstTime  \\\n",
      "0         1                                                [1]   0:00:00   \n",
      "1         2                                       [1460, 1461]   0:28:54   \n",
      "2         3                                              [267]   0:06:51   \n",
      "3         4                                 [1236, 1246, 1247]   0:24:02   \n",
      "4         5                           [3459, 3462, 3463, 3464]   1:02:39   \n",
      "5         6                                       [2474, 2476]   0:43:46   \n",
      "6         7                                              [273]   0:06:54   \n",
      "7         8                                       [3467, 3469]   1:02:43   \n",
      "8         9  [3470, 3472, 3473, 3474, 3478, 3480, 3483, 348...   1:02:46   \n",
      "9        10                                       [1237, 1242]   0:24:02   \n",
      "\n",
      "   FirstFrame  FirstX  FirstY FirstZone LastTime  LastFrame   LastX  LastY  \\\n",
      "0         1.0   993.0   402.0        C2  0:00:21     1265.0   522.0  493.0   \n",
      "1    103973.0   171.0   474.0       NaN  0:28:55   104012.0   177.0  465.0   \n",
      "2     24652.0    58.0   550.0        C3  0:06:51    24667.0    22.0  549.0   \n",
      "3     86439.0    15.0   535.0       NaN  0:24:12    87072.0   315.0  394.0   \n",
      "4    225328.0  1256.0   407.0       NaN  1:02:42   225499.0  1116.0  399.0   \n",
      "5    157427.0    15.0   526.0       NaN  0:44:00   158258.0   350.0  377.0   \n",
      "6     24854.0    59.0   514.0       NaN  0:06:54    24855.0    58.0  514.0   \n",
      "7    225578.0  1053.0   398.0       NaN  1:02:44   225642.0  1054.0  392.0   \n",
      "8    225767.0   972.0   400.0        C2  1:02:58   226492.0   962.0  405.0   \n",
      "9     86480.0   749.0   378.0       NaN  0:24:04    86586.0   786.0  379.0   \n",
      "\n",
      "  LastZone  \n",
      "0       C3  \n",
      "1      NaN  \n",
      "2       C3  \n",
      "3       C4  \n",
      "4      NaN  \n",
      "5       C4  \n",
      "6      NaN  \n",
      "7      NaN  \n",
      "8      NaN  \n",
      "9      NaN  \n"
     ]
    }
   ],
   "source": [
    "# ===================== PASS 6 MERGE (Direct Euclidean Distance Merge, 6s time threshold) =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge5.csv\"\n",
    "FPS = 30  # frames per second\n",
    "DIST_MIN = 2   # minimum Euclidean distance to merge (pixels)\n",
    "DIST_MAX = 50   # maximum Euclidean distance to merge (pixels)\n",
    "TIME_THRESH_SEC = 15  # max time difference in seconds (changed from 3s)\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "TIME_MIN_SEC = 0.5  # minimum time difference in seconds\n",
    "FRAME_MIN = TIME_MIN_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"5.csv\", \"6.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"5.csv\", \"6summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID5\"] = df[\"MergeID5\"].astype(str)\n",
    "df = df.dropna(subset=[\"MergeID5\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID5\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID5\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID5\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== DIRECT MERGE LOGIC =====================\n",
    "merge_map = {}\n",
    "next_merge_id = 1\n",
    "used_nodes = set()\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    if row_a.MergeID5 in used_nodes:\n",
    "        continue\n",
    "\n",
    "    # Find valid candidates within thresholds\n",
    "    candidates = []\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID5 == row_b.MergeID5 or row_b.MergeID5 in used_nodes:\n",
    "            continue\n",
    "\n",
    "        dist = np.sqrt((row_a.X_last - row_b.X_first)**2 + (row_a.Y_last - row_b.Y_first)**2)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if DIST_MIN <= dist <= DIST_MAX and FRAME_MIN <= frame_diff <= FRAME_THRESH:\n",
    "            candidates.append((dist, row_b.MergeID5))\n",
    "\n",
    "    # Merge with the closest candidate only (or keep alone)\n",
    "    merge_map[row_a.MergeID5] = next_merge_id\n",
    "    used_nodes.add(row_a.MergeID5)\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort()  # sort by distance\n",
    "        closest = candidates[0][1]\n",
    "        merge_map[closest] = next_merge_id\n",
    "        used_nodes.add(closest)\n",
    "\n",
    "    next_merge_id += 1\n",
    "\n",
    "# ===================== MAP BACK TO FULL DATA =====================\n",
    "df[\"MergeID6\"] = df[\"MergeID5\"].map(merge_map).fillna(-1).astype(int)\n",
    "df = df.sort_values([\"MergeID6\", \"MergeID5\", \"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID6 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID6\"].unique()):\n",
    "    subset = df[df[\"MergeID6\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID6\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 6 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb0c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 7 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge7.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge7summary.csv\n",
      "   MergeID7            PersonID FirstTime  FirstFrame  FirstX  FirstY  \\\n",
      "0         1         [1, 19, 23]   0:00:00         1.0   993.0   402.0   \n",
      "1         2        [1237, 1242]   0:24:02     86480.0   749.0   378.0   \n",
      "2         3                [51]   0:01:41      6080.0   526.0   460.0   \n",
      "3         4              [2499]   0:43:59    158190.0  1036.0   393.0   \n",
      "4         5  [1265, 1266, 1268]   0:24:16     87331.0  1044.0   396.0   \n",
      "5         6          [896, 954]   0:19:25     69862.0   428.0   508.0   \n",
      "6         7          [510, 514]   0:13:13     47554.0   944.0   395.0   \n",
      "7         8               [511]   0:13:13     47563.0   871.0   393.0   \n",
      "8         9               [516]   0:13:16     47747.0   452.0   469.0   \n",
      "9        10              [2742]   0:48:05    172954.0   531.0   473.0   \n",
      "\n",
      "  FirstZone LastTime  LastFrame   LastX  LastY LastZone  \n",
      "0        C2  0:00:29     1759.0    13.0  535.0      NaN  \n",
      "1       NaN  0:24:04    86586.0   786.0  379.0      NaN  \n",
      "2       NaN  0:01:46     6404.0  1264.0  409.0      NaN  \n",
      "3       NaN  0:44:00   158271.0  1113.0  378.0      NaN  \n",
      "4       NaN  0:24:21    87580.0  1009.0  403.0       C2  \n",
      "5        C3  0:19:59    71879.0   527.0  502.0       C3  \n",
      "6        C2  0:13:23    48175.0   530.0  485.0       C3  \n",
      "7       NaN  0:13:16    47740.0   515.0  441.0      NaN  \n",
      "8        C3  0:13:17    47820.0    34.0  507.0       C3  \n",
      "9       NaN  0:48:09   173194.0   727.0  415.0      NaN  \n"
     ]
    }
   ],
   "source": [
    "# ===================== PASS 7 MERGE (Direct Euclidean Distance Merge, ) =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge6.csv\"\n",
    "FPS = 30  # frames per second\n",
    "DIST_MIN = 0   # minimum Euclidean distance to merge (pixels)\n",
    "DIST_MAX = 60   # maximum Euclidean distance to merge (pixels)\n",
    "TIME_THRESH_SEC = 10  # max time difference in seconds (changed from 3s)\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "TIME_MIN_SEC = 0.5  # minimum time difference in seconds\n",
    "FRAME_MIN = TIME_MIN_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"6.csv\", \"7.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"6.csv\", \"7summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID6\"] = df[\"MergeID6\"].astype(str)\n",
    "df = df.dropna(subset=[\"MergeID6\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID6\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID6\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID6\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== DIRECT MERGE LOGIC =====================\n",
    "merge_map = {}\n",
    "next_merge_id = 1\n",
    "used_nodes = set()\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    if row_a.MergeID6 in used_nodes:\n",
    "        continue\n",
    "\n",
    "    # Find valid candidates within thresholds\n",
    "    candidates = []\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID6 == row_b.MergeID6 or row_b.MergeID6 in used_nodes:\n",
    "            continue\n",
    "\n",
    "        dist = np.sqrt((row_a.X_last - row_b.X_first)**2 + (row_a.Y_last - row_b.Y_first)**2)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if DIST_MIN <= dist <= DIST_MAX and FRAME_MIN <= frame_diff <= FRAME_THRESH:\n",
    "            candidates.append((dist, row_b.MergeID6))\n",
    "\n",
    "    # Merge with the closest candidate only (or keep alone)\n",
    "    merge_map[row_a.MergeID6] = next_merge_id\n",
    "    used_nodes.add(row_a.MergeID6)\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort()  # sort by distance\n",
    "        closest = candidates[0][1]\n",
    "        merge_map[closest] = next_merge_id\n",
    "        used_nodes.add(closest)\n",
    "\n",
    "    next_merge_id += 1\n",
    "\n",
    "# ===================== MAP BACK TO FULL DATA =====================\n",
    "df[\"MergeID7\"] = df[\"MergeID6\"].map(merge_map).fillna(-1).astype(int)\n",
    "df = df.sort_values([\"MergeID7\", \"MergeID6\", \"MergeID5\", \"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID7 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID7\"].unique()):\n",
    "    subset = df[df[\"MergeID7\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID7\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 7 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b15a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 8 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge8.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge8summary.csv\n",
      "   MergeID8                                           PersonID FirstTime  \\\n",
      "0         1                                 [1, 9, 19, 23, 27]   0:00:00   \n",
      "1         2                                             [2742]   0:48:05   \n",
      "2         3                                       [1615, 1761]   0:31:13   \n",
      "3         4                                         [969, 978]   0:20:08   \n",
      "4         5                                              [972]   0:20:08   \n",
      "5         6                                              [975]   0:20:09   \n",
      "6         7                                    [979, 983, 985]   0:20:27   \n",
      "7         8                                              [276]   0:06:57   \n",
      "8         9                                 [2513, 2514, 2516]   0:44:44   \n",
      "9        10  [2536, 2537, 2539, 2541, 2543, 2546, 2553, 255...   0:45:32   \n",
      "\n",
      "   FirstFrame  FirstX  FirstY FirstZone LastTime  LastFrame   LastX  LastY  \\\n",
      "0         1.0   993.0   402.0        C2  0:00:38     2293.0    87.0  471.0   \n",
      "1    172954.0   531.0   473.0       NaN  0:48:09   173194.0   727.0  415.0   \n",
      "2    112270.0    18.0   528.0       NaN  0:33:10   119289.0   473.0  496.0   \n",
      "3     72443.0   458.0   506.0        C3  0:20:12    72664.0    14.0  517.0   \n",
      "4     72460.0   458.0   480.0        C3  0:20:12    72658.0    29.0  516.0   \n",
      "5     72505.0  1176.0   399.0       NaN  0:20:10    72550.0  1219.0  390.0   \n",
      "6     73558.0   341.0   389.0        C4  0:20:35    74049.0   336.0  389.0   \n",
      "7     25032.0   463.0   492.0        C3  0:07:25    26732.0   254.0  407.0   \n",
      "8    160911.0   986.0   392.0        C2  0:44:55   161552.0   527.0  464.0   \n",
      "9    163808.0   818.0   385.0       NaN  0:46:08   165970.0   977.0  373.0   \n",
      "\n",
      "  LastZone  \n",
      "0       C3  \n",
      "1      NaN  \n",
      "2       C3  \n",
      "3      NaN  \n",
      "4       C3  \n",
      "5      NaN  \n",
      "6       C4  \n",
      "7      NaN  \n",
      "8      NaN  \n",
      "9       C2  \n"
     ]
    }
   ],
   "source": [
    "# ===================== PASS 8 MERGE (Direct Euclidean Distance Merge, ) =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge7.csv\"\n",
    "FPS = 30  # frames per second\n",
    "DIST_MIN = 0   # minimum Euclidean distance to merge (pixels)\n",
    "DIST_MAX = 90   # maximum Euclidean distance to merge (pixels)\n",
    "TIME_THRESH_SEC = 10  # max time difference in seconds (changed from 3s)\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "TIME_MIN_SEC = 0.5  # minimum time difference in seconds\n",
    "FRAME_MIN = TIME_MIN_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"7.csv\", \"8.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"7.csv\", \"8summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID7\"] = df[\"MergeID7\"].astype(str)\n",
    "df = df.dropna(subset=[\"MergeID7\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID7\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID7\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID7\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== DIRECT MERGE LOGIC =====================\n",
    "merge_map = {}\n",
    "next_merge_id = 1\n",
    "used_nodes = set()\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    if row_a.MergeID7 in used_nodes:\n",
    "        continue\n",
    "\n",
    "    # Find valid candidates within thresholds\n",
    "    candidates = []\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID7 == row_b.MergeID7 or row_b.MergeID7 in used_nodes:\n",
    "            continue\n",
    "\n",
    "        dist = np.sqrt((row_a.X_last - row_b.X_first)**2 + (row_a.Y_last - row_b.Y_first)**2)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if DIST_MIN <= dist <= DIST_MAX and FRAME_MIN <= frame_diff <= FRAME_THRESH:\n",
    "            candidates.append((dist, row_b.MergeID7))\n",
    "\n",
    "    # Merge with the closest candidate only (or keep alone)\n",
    "    merge_map[row_a.MergeID7] = next_merge_id\n",
    "    used_nodes.add(row_a.MergeID7)\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort()  # sort by distance\n",
    "        closest = candidates[0][1]\n",
    "        merge_map[closest] = next_merge_id\n",
    "        used_nodes.add(closest)\n",
    "\n",
    "    next_merge_id += 1\n",
    "\n",
    "# ===================== MAP BACK TO FULL DATA =====================\n",
    "df[\"MergeID8\"] = df[\"MergeID7\"].map(merge_map).fillna(-1).astype(int)\n",
    "df = df.sort_values([\"MergeID8\", \"MergeID7\", \"MergeID6\", \"MergeID5\", \"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID8 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID8\"].unique()):\n",
    "    subset = df[df[\"MergeID8\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID8\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 8 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378d87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 9 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge9.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge9summary.csv\n",
      "   MergeID9                                           PersonID FirstTime  \\\n",
      "0         1                                 [1, 9, 19, 23, 27]   0:00:00   \n",
      "1         2  [2536, 2537, 2539, 2541, 2543, 2546, 2553, 255...   0:45:32   \n",
      "2         3  [372, 373, 379, 388, 394, 398, 401, 406, 407, ...   0:10:10   \n",
      "3         4                                             [1051]   0:22:06   \n",
      "4         5                                             [1632]   0:31:33   \n",
      "5         6                           [1237, 1241, 1242, 1245]   0:24:02   \n",
      "6         7               [1604, 1606, 1608, 1614, 1616, 1620]   0:31:01   \n",
      "7         8                     [1048, 1103, 1134, 1137, 1154]   0:22:03   \n",
      "8         9  [80, 81, 82, 84, 85, 88, 90, 93, 97, 102, 104,...   0:02:35   \n",
      "9        10                                             [1637]   0:31:36   \n",
      "\n",
      "   FirstFrame  FirstX  FirstY FirstZone LastTime  LastFrame  LastX  LastY  \\\n",
      "0         1.0   993.0   402.0        C2  0:00:38     2293.0   87.0  471.0   \n",
      "1    163808.0   818.0   385.0       NaN  0:46:13   166272.0  976.0  375.0   \n",
      "2     36578.0   964.0   376.0       NaN  0:10:51    39032.0  873.0  393.0   \n",
      "3     79529.0   958.0   387.0        C2  0:22:06    79536.0  953.0  389.0   \n",
      "4    113498.0   768.0   425.0       NaN  0:31:33   113502.0  752.0  424.0   \n",
      "5     86480.0   749.0   378.0       NaN  0:24:05    86632.0  868.0  383.0   \n",
      "6    111584.0   967.0   395.0        C2  0:31:18   112590.0  969.0  369.0   \n",
      "7     79331.0    51.0   717.0       NaN  0:23:26    84301.0  821.0  414.0   \n",
      "8      9314.0  1010.0   392.0        C2  0:03:31    12703.0  116.0  717.0   \n",
      "9    113674.0   964.0   389.0        C2  0:31:37   113745.0  944.0  385.0   \n",
      "\n",
      "  LastZone  \n",
      "0       C3  \n",
      "1       C2  \n",
      "2      NaN  \n",
      "3       C2  \n",
      "4      NaN  \n",
      "5      NaN  \n",
      "6       C2  \n",
      "7      NaN  \n",
      "8      NaN  \n",
      "9       C2  \n"
     ]
    }
   ],
   "source": [
    "# ===================== PASS 9 MERGE (Direct Euclidean Distance Merge, ) =====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/5AM-arrivaltimezone-merge8.csv\"\n",
    "FPS = 30  # frames per second\n",
    "DIST_MIN = 0   # minimum Euclidean distance to merge (pixels)\n",
    "DIST_MAX = 150   # maximum Euclidean distance to merge (pixels)\n",
    "TIME_THRESH_SEC = 10  # max time difference in seconds (changed from 3s)\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "TIME_MIN_SEC = 1  # minimum time difference in seconds\n",
    "FRAME_MIN = TIME_MIN_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"8.csv\", \"9.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"8.csv\", \"9summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID8\"] = df[\"MergeID8\"].astype(str)\n",
    "df = df.dropna(subset=[\"MergeID8\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID8\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID8\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID8\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== DIRECT MERGE LOGIC =====================\n",
    "merge_map = {}\n",
    "next_merge_id = 1\n",
    "used_nodes = set()\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    if row_a.MergeID8 in used_nodes:\n",
    "        continue\n",
    "\n",
    "    # Find valid candidates within thresholds\n",
    "    candidates = []\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID8 == row_b.MergeID8 or row_b.MergeID8 in used_nodes:\n",
    "            continue\n",
    "\n",
    "        dist = np.sqrt((row_a.X_last - row_b.X_first)**2 + (row_a.Y_last - row_b.Y_first)**2)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if DIST_MIN <= dist <= DIST_MAX and FRAME_MIN <= frame_diff <= FRAME_THRESH:\n",
    "            candidates.append((dist, row_b.MergeID8))\n",
    "\n",
    "    # Merge with the closest candidate only (or keep alone)\n",
    "    merge_map[row_a.MergeID8] = next_merge_id\n",
    "    used_nodes.add(row_a.MergeID8)\n",
    "\n",
    "    if candidates:\n",
    "        candidates.sort()  # sort by distance\n",
    "        closest = candidates[0][1]\n",
    "        merge_map[closest] = next_merge_id\n",
    "        used_nodes.add(closest)\n",
    "\n",
    "    next_merge_id += 1\n",
    "\n",
    "# ===================== MAP BACK TO FULL DATA =====================\n",
    "df[\"MergeID9\"] = df[\"MergeID8\"].map(merge_map).fillna(-1).astype(int)\n",
    "df = df.sort_values([\"MergeID9\", \"MergeID8\", \"MergeID7\", \"MergeID6\", \"MergeID5\", \"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID9 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID9\"].unique()):\n",
    "    subset = df[df[\"MergeID9\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID9\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 9 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cc6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 4 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial22-arrivaltimezone-merge4.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial22-arrivaltimezone-merge4summary.csv\n",
      "      Time  PersonID  Frame      X      Y Zone Direction ArrivalTime  \\\n",
      "0  0:00:00         1    1.0  525.0  371.0   C4       NaN     0:00:00   \n",
      "1  0:00:00         1    3.0  525.0  370.0   C4       NaN     0:00:00   \n",
      "2  0:00:00         1    6.0  525.0  371.0   C4       NaN     0:00:00   \n",
      "3  0:00:00         1    8.0  525.0  370.0   C4       NaN     0:00:00   \n",
      "4  0:00:00         1   17.0  525.0  371.0   C4       NaN     0:00:00   \n",
      "5  0:00:00         1   26.0  525.0  370.0   C4       NaN     0:00:00   \n",
      "6  0:00:00         1   32.0  525.0  371.0   C4       NaN     0:00:00   \n",
      "7  0:00:00         1   33.0  525.0  370.0   C4       NaN     0:00:00   \n",
      "8  0:00:00         1   46.0  525.0  371.0   C4       NaN     0:00:00   \n",
      "9  0:00:00         1   50.0  525.0  370.0   C4       NaN     0:00:00   \n",
      "\n",
      "  ArrivalZone MergeID MergeID2 MergeID3  MergeID4  \n",
      "0          C4       1        1        1         1  \n",
      "1          C4       1        1        1         1  \n",
      "2          C4       1        1        1         1  \n",
      "3          C4       1        1        1         1  \n",
      "4          C4       1        1        1         1  \n",
      "5          C4       1        1        1         1  \n",
      "6          C4       1        1        1         1  \n",
      "7          C4       1        1        1         1  \n",
      "8          C4       1        1        1         1  \n",
      "9          C4       1        1        1         1  \n"
     ]
    }
   ],
   "source": [
    "#pass 4 : merge 4\n",
    "# ===================== PASS 4 MERGE (Bounding Box Method) =====================\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial22-arrivaltimezone-merge3.csv\"\n",
    "FPS = 30  # frames per second\n",
    "X_THRESH = 25  # max difference in X (pixels)\n",
    "Y_THRESH = 20  # max difference in Y (pixels)\n",
    "TIME_THRESH_SEC = 2  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"3.csv\", \"4.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"3.csv\", \"4summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID\"] = df[\"MergeID\"].astype(str)\n",
    "df[\"MergeID2\"] = df[\"MergeID2\"].astype(str)\n",
    "df[\"MergeID3\"] = df[\"MergeID3\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID3\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID3\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID3\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID3 is a node\n",
    "for mid in first_last_df[\"MergeID3\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID3 == row_b.MergeID3:\n",
    "            continue\n",
    "\n",
    "        # Bounding box proximity: last of A to first of B\n",
    "        dx = abs(row_a.X_last - row_b.X_first)\n",
    "        dy = abs(row_a.Y_last - row_b.Y_first)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID3, row_b.MergeID3)\n",
    "\n",
    "# ===================== ASSIGN PASS 4 MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for mid in comp:\n",
    "        merge_map[mid] = merge_id\n",
    "\n",
    "# Map new MergeID back to full dataset\n",
    "df[\"MergeID4\"] = df[\"MergeID3\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "# df[\"MergeID2\"] = pd.to_numeric(df[\"MergeID2\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Sort and reorder columns\n",
    "df = df.sort_values([\"MergeID4\", \"MergeID3\", \"MergeID2\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "# cols = [\"MergeID2\"] + [c for c in df.columns if c != \"MergeID2\"]\n",
    "# df = df[cols]\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID4 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID4\"].unique()):\n",
    "    subset = df[df[\"MergeID4\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID4\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 4 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8ab52ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pass 3 merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial22-arrivaltimezone-merge3.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial22-arrivaltimezone-merge3summary.csv\n",
      "   MergeID3 MergeID2  MergeID     Time  PersonID  Frame      X      Y Zone  \\\n",
      "0         1        1        1  0:00:00         1    1.0  525.0  371.0   C4   \n",
      "1         1        1        1  0:00:00         1    3.0  525.0  370.0   C4   \n",
      "2         1        1        1  0:00:00         1    6.0  525.0  371.0   C4   \n",
      "3         1        1        1  0:00:00         1    8.0  525.0  370.0   C4   \n",
      "4         1        1        1  0:00:00         1   17.0  525.0  371.0   C4   \n",
      "5         1        1        1  0:00:00         1   26.0  525.0  370.0   C4   \n",
      "6         1        1        1  0:00:00         1   32.0  525.0  371.0   C4   \n",
      "7         1        1        1  0:00:00         1   33.0  525.0  370.0   C4   \n",
      "8         1        1        1  0:00:00         1   46.0  525.0  371.0   C4   \n",
      "9         1        1        1  0:00:00         1   50.0  525.0  370.0   C4   \n",
      "\n",
      "  Direction ArrivalTime ArrivalZone  \n",
      "0       NaN     0:00:00          C4  \n",
      "1       NaN     0:00:00          C4  \n",
      "2       NaN     0:00:00          C4  \n",
      "3       NaN     0:00:00          C4  \n",
      "4       NaN     0:00:00          C4  \n",
      "5       NaN     0:00:00          C4  \n",
      "6       NaN     0:00:00          C4  \n",
      "7       NaN     0:00:00          C4  \n",
      "8       NaN     0:00:00          C4  \n",
      "9       NaN     0:00:00          C4  \n"
     ]
    }
   ],
   "source": [
    "# pass 3 : merge 3\n",
    "# ===================== PASS 3 MERGE (Bounding Box Method) =====================\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial22-arrivaltimezone-merge2.csv\"\n",
    "FPS = 30  # frames per second\n",
    "X_THRESH = 30  # max difference in X (pixels)\n",
    "Y_THRESH = 20  # max difference in Y (pixels)\n",
    "TIME_THRESH_SEC = 5  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\"2.csv\", \"3.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\"2.csv\", \"3summary.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"MergeID2\"] = df[\"MergeID2\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"MergeID2\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"MergeID2\").first().reset_index()\n",
    "last_obs = df.groupby(\"MergeID2\").last().reset_index()\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"MergeID2\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID2 is a node\n",
    "for mid in first_last_df[\"MergeID2\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.MergeID2 == row_b.MergeID2:\n",
    "            continue\n",
    "\n",
    "        # Bounding box proximity: last of A to first of B\n",
    "        dx = abs(row_a.X_last - row_b.X_first)\n",
    "        dy = abs(row_a.Y_last - row_b.Y_first)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID2, row_b.MergeID2)\n",
    "\n",
    "# ===================== ASSIGN PASS 3 MERGEID3s =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for mid in comp:\n",
    "        merge_map[mid] = merge_id\n",
    "\n",
    "# Map new MergeID2 back to full dataset\n",
    "df[\"MergeID3\"] = df[\"MergeID2\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "df[\"PersonID\"] = pd.to_numeric(df[\"PersonID\"], errors=\"coerce\").astype(int)\n",
    "df[\"MergeID3\"] = pd.to_numeric(df[\"MergeID3\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Sort and reorder columns\n",
    "df = df.sort_values([\"MergeID3\", \"MergeID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "cols = [\"MergeID3\"] + [c for c in df.columns if c != \"MergeID3\"]\n",
    "df = df[cols]\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID3 =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID3\"].unique()):\n",
    "    subset = df[df[\"MergeID3\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID3\": merge_id,\n",
    "        \"PersonID\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"LastTime\": last_row[\"Time\"],\n",
    "        \"LastFrame\": last_row[\"Frame\"],\n",
    "        \"LastX\": last_row[\"X\"],\n",
    "        \"LastY\": last_row[\"Y\"],\n",
    "        \"LastZone\": last_row.get(\"Zone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved pass 3 merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c24fe71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved third-pass merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-summary1-summary2-mergeID3.csv\n",
      "Saved third-pass summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-summary1-summary2-summary3.csv\n"
     ]
    }
   ],
   "source": [
    "#pass three\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-summary1-summary2.csv\"\n",
    "FPS = 30\n",
    "X_THRESH = 100  # spatial threshold for third pass\n",
    "Y_THRESH = 100\n",
    "TIME_THRESH_SEC = 6  # temporal threshold\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\".csv\", \"-mergeID3.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\".csv\", \"-summary3.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df.dropna(subset=[\"FirstFrame\", \"FirstX\", \"FirstY\", \"LastFrame\", \"LastX\", \"LastY\"])\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID_2 is a node\n",
    "for mid in df[\"MergeID_2\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in df.iterrows():\n",
    "    for idx_b, row_b in df.iterrows():\n",
    "        if row_a.MergeID_2 == row_b.MergeID_2:\n",
    "            continue\n",
    "\n",
    "        dx = abs(row_a.LastX - row_b.FirstX)\n",
    "        dy = abs(row_a.LastY - row_b.FirstY)\n",
    "        frame_diff = abs(row_a.LastFrame - row_b.FirstFrame)\n",
    "\n",
    "        # Merge if within thresholds\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID_2, row_b.MergeID_2)\n",
    "\n",
    "# ===================== ASSIGN NEW MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for new_merge_id, comp in enumerate(components, start=1):\n",
    "    for old_merge_id in comp:\n",
    "        merge_map[old_merge_id] = new_merge_id\n",
    "\n",
    "df[\"MergeID_3\"] = df[\"MergeID_2\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# ===================== SAVE THIRD PASS =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID_3\"].unique()):\n",
    "    subset = df[df[\"MergeID_3\"] == merge_id].sort_values(by=\"FirstFrame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "\n",
    "    merged_persons = []\n",
    "    for p in subset[\"PersonIDs\"]:\n",
    "        if isinstance(p, str):\n",
    "            merged_persons.extend(eval(p))\n",
    "        else:\n",
    "            merged_persons.append(p)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID_3\": merge_id,\n",
    "        \"PersonIDs\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"FirstTime\"],\n",
    "        \"FirstFrame\": first_row[\"FirstFrame\"],\n",
    "        \"FirstX\": first_row[\"FirstX\"],\n",
    "        \"FirstY\": first_row[\"FirstY\"],\n",
    "        \"FirstZone\": first_row.get(\"FirstZone\", \"\"),\n",
    "        \"LastTime\": last_row[\"LastTime\"],\n",
    "        \"LastFrame\": last_row[\"LastFrame\"],\n",
    "        \"LastX\": last_row[\"LastX\"],\n",
    "        \"LastY\": last_row[\"LastY\"],\n",
    "        \"LastZone\": last_row.get(\"LastZone\", \"\")\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved third-pass merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved third-pass summary → {OUTPUT_SUMMARY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300df672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned table saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-summary1-summary2-summary3-cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "#final clean up - delete row if time is 1s or less\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_FILE = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-summary1-summary2-summary3.csv\"\n",
    "OUTPUT_FILE = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial20-postprocess-summary1-summary2-summary3-cleaned.csv\"\n",
    "\n",
    "# ===================== LOAD =====================\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# ===================== CONVERT TIMES =====================\n",
    "# Convert to timedelta for comparison\n",
    "df[\"FirstTime_td\"] = pd.to_timedelta(df[\"FirstTime\"])\n",
    "df[\"LastTime_td\"] = pd.to_timedelta(df[\"LastTime\"])\n",
    "\n",
    "# ===================== FILTER ROWS =====================\n",
    "# Keep only rows where duration > 0 seconds\n",
    "df_clean = df[df[\"LastTime_td\"] != df[\"FirstTime_td\"]].copy()\n",
    "\n",
    "# ===================== REMOVE TEMP COLUMNS =====================\n",
    "df_clean = df_clean.drop(columns=[\"FirstTime_td\", \"LastTime_td\"], errors=\"ignore\")\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "df_clean.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Cleaned table saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2547873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5220695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full detection log with arrival info → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-arrivaldeparture.csv\n"
     ]
    }
   ],
   "source": [
    "#add arrival and departure time\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13.csv\"\n",
    "OUTPUT_CSV = INPUT_CSV.replace(\".csv\", \"-arrivaldeparture.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Ensure proper types\n",
    "df[\"PersonID\"] = df[\"PersonID\"].astype(str)\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"PixelX\"] = pd.to_numeric(df[\"PixelX\"], errors=\"coerce\")\n",
    "df[\"PixelY\"] = pd.to_numeric(df[\"PixelY\"], errors=\"coerce\")\n",
    "df[\"Zone\"] = df[\"Zone\"].astype(str)\n",
    "\n",
    "# ===================== COMPUTE ARRIVAL & DEPARTURE =====================\n",
    "arrival_info = {}\n",
    "\n",
    "for pid, group in df.groupby(\"PersonID\"):\n",
    "    # Only rows where Zone is assigned\n",
    "    zone_rows = group[group[\"Zone\"].notna() & (group[\"Zone\"] != \"\")]\n",
    "    if not zone_rows.empty:\n",
    "        first_zone_row = zone_rows.iloc[0]\n",
    "        last_zone_row = zone_rows.iloc[-1]\n",
    "        arrival_info[pid] = {\n",
    "            \"ArrivalTime\": first_zone_row[\"Timestamp\"],\n",
    "            \"ArrivalZone\": first_zone_row[\"Zone\"],\n",
    "            \"DepartureTime\": last_zone_row[\"Timestamp\"]\n",
    "        }\n",
    "    else:\n",
    "        arrival_info[pid] = {\n",
    "            \"ArrivalTime\": None,\n",
    "            \"ArrivalZone\": None,\n",
    "            \"DepartureTime\": None\n",
    "        }\n",
    "\n",
    "# ===================== ADD COLUMNS TO FULL DATA =====================\n",
    "df[\"ArrivalTime\"] = df[\"PersonID\"].map(lambda pid: arrival_info[pid][\"ArrivalTime\"])\n",
    "df[\"ArrivalZone\"] = df[\"PersonID\"].map(lambda pid: arrival_info[pid][\"ArrivalZone\"])\n",
    "df[\"DepartureTime\"] = df[\"PersonID\"].map(lambda pid: arrival_info[pid][\"DepartureTime\"])\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved full detection log with arrival info → {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2045fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First & last observation CSV saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess.csv\n"
     ]
    }
   ],
   "source": [
    "# post processing\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-arrivaldeparture.csv\"\n",
    "OUTPUT_CSV = INPUT_CSV.replace(\"-arrivaldeparture.csv\", \"-postprocess.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df[\"PersonID\"] = df[\"PersonID\"].astype(str)\n",
    "\n",
    "# ===================== FILTER FIRST & LAST =====================\n",
    "rows = []\n",
    "for pid, group in df.groupby(\"PersonID\"):\n",
    "    group_sorted = group.sort_values(\"Frame\")\n",
    "    first_row = group_sorted.iloc[0]\n",
    "    last_row = group_sorted.iloc[-1]\n",
    "    rows.append(first_row)\n",
    "    if len(group_sorted) > 1:  # only add last row if different from first\n",
    "        rows.append(last_row)\n",
    "\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"First & last observation CSV saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efbb0660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processed merged CSV saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-postprocess-mergedIDs.csv\n"
     ]
    }
   ],
   "source": [
    "#post porcessing unification (can delete) - use to check?\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess.csv\"\n",
    "OUTPUT_CSV = INPUT_CSV.replace(\".csv\", \"-postprocess-mergedIDs.csv\")\n",
    "\n",
    "DIST_MIN = 70    # minimum distance threshold\n",
    "DIST_MAX = 100   # maximum distance threshold\n",
    "\n",
    "# ===================== LOAD =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Sort by PersonID then Frame\n",
    "df = df.sort_values([\"PersonID\", \"Frame\"]).reset_index(drop=True)\n",
    "\n",
    "# Keep a mapping of final merged IDs\n",
    "id_mapping = {}\n",
    "\n",
    "# Helper function: Euclidean distance\n",
    "def distance(p1, p2):\n",
    "    return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "# ===================== PROCESS =====================\n",
    "# Store last known position of each merged ID\n",
    "last_positions = {}\n",
    "\n",
    "for pid, group in df.groupby(\"PersonID\"):\n",
    "    first_row = group.iloc[0]\n",
    "    last_row = group.iloc[-1]\n",
    "\n",
    "    new_id = pid\n",
    "    first_pos = (first_row[\"PixelX\"], first_row[\"PixelY\"])\n",
    "    first_frame = first_row[\"Frame\"]\n",
    "\n",
    "    best_match = None\n",
    "    best_dist = float(\"inf\")\n",
    "\n",
    "    # Check all existing merged IDs\n",
    "    for existing_id, (ex_frame, ex_pos) in last_positions.items():\n",
    "        if ex_frame < first_frame:  # only look at earlier tracks\n",
    "            d = distance(first_pos, ex_pos)\n",
    "            if DIST_MIN <= d <= DIST_MAX and d < best_dist:\n",
    "                best_match = existing_id\n",
    "                best_dist = d\n",
    "\n",
    "    if best_match is not None:\n",
    "        # Merge: map this PersonID to existing one\n",
    "        id_mapping[new_id] = best_match\n",
    "        # Update last position of matched ID with this new track’s last row\n",
    "        last_positions[best_match] = (last_row[\"Frame\"], (last_row[\"PixelX\"], last_row[\"PixelY\"]))\n",
    "    else:\n",
    "        # Keep as new identity\n",
    "        id_mapping[new_id] = new_id\n",
    "        last_positions[new_id] = (last_row[\"Frame\"], (last_row[\"PixelX\"], last_row[\"PixelY\"]))\n",
    "\n",
    "# ===================== APPLY MERGE =====================\n",
    "df[\"MergedID\"] = df[\"PersonID\"].map(id_mapping)\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Post-processed merged CSV saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c340cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-mergeID1.csv\n",
      "Saved summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1.csv\n"
     ]
    }
   ],
   "source": [
    "#pass 1\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess.csv\"\n",
    "FPS = 30  # frames per second (needed to convert seconds to frames)\n",
    "X_THRESH = 40  # pixels\n",
    "Y_THRESH = 40  # pixels\n",
    "TIME_THRESH_SEC = 2  # max time difference in seconds\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\".csv\", \"-mergeID1.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\".csv\", \"-summary1.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV, header=None, names=[\"Time\", \"PersonID\", \"Frame\", \"X\", \"Y\", \"Zone\"])\n",
    "\n",
    "# Convert numeric columns\n",
    "df[\"Frame\"] = pd.to_numeric(df[\"Frame\"], errors=\"coerce\")\n",
    "df[\"X\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
    "df[\"Y\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
    "df[\"PersonID\"] = df[\"PersonID\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"PersonID\", \"Frame\", \"X\", \"Y\"])\n",
    "\n",
    "# ===================== FIRST & LAST OBSERVATIONS =====================\n",
    "first_obs = df.groupby(\"PersonID\").first().reset_index()\n",
    "last_obs = df.groupby(\"PersonID\").last().reset_index()\n",
    "\n",
    "# Merge first and last for easier comparison\n",
    "first_last_df = pd.merge(first_obs, last_obs, on=\"PersonID\", suffixes=(\"_first\", \"_last\"))\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each PersonID is a node\n",
    "for pid in first_last_df[\"PersonID\"]:\n",
    "    G.add_node(pid)\n",
    "\n",
    "for idx_a, row_a in first_last_df.iterrows():\n",
    "    for idx_b, row_b in first_last_df.iterrows():\n",
    "        if row_a.PersonID == row_b.PersonID:\n",
    "            continue\n",
    "\n",
    "        # Spatial proximity: last of A to first of B\n",
    "        dx = abs(row_a.X_last - row_b.X_first)\n",
    "        dy = abs(row_a.Y_last - row_b.Y_first)\n",
    "        frame_diff = abs(row_a.Frame_last - row_b.Frame_first)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            # Link these two IDs\n",
    "            G.add_edge(row_a.PersonID, row_b.PersonID)\n",
    "\n",
    "# ===================== ASSIGN MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for merge_id, comp in enumerate(components, start=1):\n",
    "    for pid in comp:\n",
    "        merge_map[pid] = merge_id\n",
    "\n",
    "# Assign MergeID back to full dataset\n",
    "df[\"MergeID\"] = df[\"PersonID\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# ===================== SAVE MERGED DATA =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY PER MergeID =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID\"].unique()):\n",
    "    subset = df[df[\"MergeID\"] == merge_id].sort_values(by=\"Frame\")\n",
    "    merged_persons = sorted(subset[\"PersonID\"].unique())\n",
    "\n",
    "    # Filter rows with non-empty zone\n",
    "    zone_rows = subset[subset[\"Zone\"].notna() & (subset[\"Zone\"].astype(str) != \"\")]\n",
    "\n",
    "    if not zone_rows.empty:\n",
    "        # Arrival: first zone\n",
    "        arrival_row = zone_rows.iloc[0]\n",
    "        arrival_time = arrival_row[\"Time\"]\n",
    "        arrival_zone = arrival_row[\"Zone\"]\n",
    "\n",
    "        # Departure: last time in same arrival zone\n",
    "        same_zone_rows = zone_rows[zone_rows[\"Zone\"] == arrival_zone]\n",
    "        departure_time = same_zone_rows.iloc[-1][\"Time\"]\n",
    "\n",
    "        # Last time / last zone: last non-empty zone in subset\n",
    "        last_row = zone_rows.iloc[-1]\n",
    "        last_time = last_row[\"Time\"]\n",
    "        last_zone = last_row[\"Zone\"]\n",
    "    else:\n",
    "        arrival_time, arrival_zone, departure_time, last_time, last_zone = None, None, None, None, None\n",
    "\n",
    "    first_row = subset.iloc[0]\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID\": merge_id,\n",
    "        \"PersonIDs\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"Time\"],\n",
    "        \"FirstFrame\": first_row[\"Frame\"],\n",
    "        \"FirstX\": first_row[\"X\"],\n",
    "        \"FirstY\": first_row[\"Y\"],\n",
    "        \"FirstZone\": first_row.get(\"Zone\", \"\"),\n",
    "        \"ArrivalTime\": arrival_time,\n",
    "        \"ArrivalZone\": arrival_zone,\n",
    "        \"DepartureTime\": departure_time,\n",
    "        \"LastTime\": last_time,\n",
    "        \"LastFrame\": subset[\"Frame\"].iloc[-1],\n",
    "        \"LastX\": subset[\"X\"].iloc[-1],\n",
    "        \"LastY\": subset[\"Y\"].iloc[-1],\n",
    "        \"LastZone\": last_zone\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved summary → {OUTPUT_SUMMARY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e26a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved second-pass merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-mergeID2.csv\n",
      "Saved second-pass summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2.csv\n",
      "Saved second-pass merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-mergeID2.csv\n",
      "Saved second-pass summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2.csv\n"
     ]
    }
   ],
   "source": [
    "#passs 2\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1.csv\"\n",
    "FPS = 30\n",
    "X_THRESH = 80  # larger threshold\n",
    "Y_THRESH = 80\n",
    "TIME_THRESH_SEC = 5  # larger temporal threshold\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\".csv\", \"-mergeID2.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\".csv\", \"-summary2.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Use first/last positions from first pass\n",
    "df = df.dropna(subset=[\"FirstFrame\", \"FirstX\", \"FirstY\", \"LastFrame\", \"LastX\", \"LastY\"])\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON MERGEID FINAL→FIRST =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID is a node\n",
    "for mid in df[\"MergeID\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in df.iterrows():\n",
    "    for idx_b, row_b in df.iterrows():\n",
    "        if row_a.MergeID == row_b.MergeID:\n",
    "            continue\n",
    "\n",
    "        dx = abs(row_a.LastX - row_b.FirstX)\n",
    "        dy = abs(row_a.LastY - row_b.FirstY)\n",
    "        frame_diff = abs(row_a.LastFrame - row_b.FirstFrame)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID, row_b.MergeID)\n",
    "\n",
    "# ===================== ASSIGN NEW MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for new_merge_id, comp in enumerate(components, start=1):\n",
    "    for old_merge_id in comp:\n",
    "        merge_map[old_merge_id] = new_merge_id\n",
    "\n",
    "# Map new MergeID to the first-pass summary\n",
    "df[\"MergeID_2\"] = df[\"MergeID\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "#passs 2\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1.csv\"\n",
    "FPS = 30\n",
    "X_THRESH = 80  # larger threshold\n",
    "Y_THRESH = 80\n",
    "TIME_THRESH_SEC = 5  # larger temporal threshold\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\".csv\", \"-mergeID2.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\".csv\", \"-summary2.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Use first/last positions from first pass\n",
    "df = df.dropna(subset=[\"FirstFrame\", \"FirstX\", \"FirstY\", \"LastFrame\", \"LastX\", \"LastY\"])\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON MERGEID FINAL→FIRST =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID is a node\n",
    "for mid in df[\"MergeID\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in df.iterrows():\n",
    "    for idx_b, row_b in df.iterrows():\n",
    "        if row_a.MergeID == row_b.MergeID:\n",
    "            continue\n",
    "\n",
    "        dx = abs(row_a.LastX - row_b.FirstX)\n",
    "        dy = abs(row_a.LastY - row_b.FirstY)\n",
    "        frame_diff = abs(row_a.LastFrame - row_b.FirstFrame)\n",
    "\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID, row_b.MergeID)\n",
    "\n",
    "# ===================== ASSIGN NEW MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for new_merge_id, comp in enumerate(components, start=1):\n",
    "    for old_merge_id in comp:\n",
    "        merge_map[old_merge_id] = new_merge_id\n",
    "\n",
    "# Map new MergeID to the first-pass summary\n",
    "df[\"MergeID_2\"] = df[\"MergeID\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID_2\"].unique()):\n",
    "    subset = df[df[\"MergeID_2\"] == merge_id].sort_values(by=\"FirstFrame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "\n",
    "    # Flatten PersonIDs\n",
    "    merged_persons = []\n",
    "    for p in subset[\"PersonIDs\"]:\n",
    "        if isinstance(p, str):\n",
    "            merged_persons.extend(ast.literal_eval(p))\n",
    "        else:\n",
    "            merged_persons.append(p)\n",
    "    merged_persons = sorted(set(merged_persons))\n",
    "\n",
    "    # Determine Arrival/Departure based on first-pass info\n",
    "    # ArrivalTime: earliest among subset\n",
    "    arrival_times = subset[\"ArrivalTime\"].dropna()\n",
    "    arrival_zones = subset[\"ArrivalZone\"].dropna()\n",
    "    departure_times = subset[\"DepartureTime\"].dropna()\n",
    "\n",
    "    arrival_time = arrival_times.min() if not arrival_times.empty else None\n",
    "    arrival_zone = arrival_zones.iloc[0] if not arrival_zones.empty else None\n",
    "    # DepartureTime: last time in the SAME zone as arrival_zone\n",
    "    if arrival_zone is not None:\n",
    "        dep_rows = subset[subset[\"ArrivalZone\"] == arrival_zone]\n",
    "        departure_time = dep_rows[\"DepartureTime\"].max() if not dep_rows.empty else None\n",
    "    else:\n",
    "        departure_time = None\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID_2\": merge_id,\n",
    "        \"PersonIDs\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"FirstTime\"],\n",
    "        \"FirstFrame\": first_row[\"FirstFrame\"],\n",
    "        \"FirstX\": first_row[\"FirstX\"],\n",
    "        \"FirstY\": first_row[\"FirstY\"],\n",
    "        \"FirstZone\": first_row.get(\"FirstZone\", \"\"),\n",
    "        \"ArrivalTime\": arrival_time,\n",
    "        \"ArrivalZone\": arrival_zone,\n",
    "        \"DepartureTime\": departure_time,\n",
    "        \"LastTime\": last_row[\"LastTime\"],\n",
    "        \"LastFrame\": last_row[\"LastFrame\"],\n",
    "        \"LastX\": last_row[\"LastX\"],\n",
    "        \"LastY\": last_row[\"LastY\"],\n",
    "        \"LastZone\": last_row.get(\"LastZone\", \"\")\n",
    "    })\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved second-pass merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved second-pass summary → {OUTPUT_SUMMARY}\")\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved second-pass merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved second-pass summary → {OUTPUT_SUMMARY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f63d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved third-pass merged dataset → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2-mergeID3.csv\n",
      "Saved third-pass summary → /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2-summary3.csv\n"
     ]
    }
   ],
   "source": [
    "#pass 3\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_CSV = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2.csv\"\n",
    "FPS = 30\n",
    "X_THRESH = 100  # spatial threshold for third pass\n",
    "Y_THRESH = 100\n",
    "TIME_THRESH_SEC = 6  # temporal threshold\n",
    "FRAME_THRESH = TIME_THRESH_SEC * FPS\n",
    "\n",
    "OUTPUT_MERGED = INPUT_CSV.replace(\".csv\", \"-mergeID3.csv\")\n",
    "OUTPUT_SUMMARY = INPUT_CSV.replace(\".csv\", \"-summary3.csv\")\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df.dropna(subset=[\"FirstFrame\", \"FirstX\", \"FirstY\", \"LastFrame\", \"LastX\", \"LastY\"])\n",
    "\n",
    "# ===================== BUILD LINKS BASED ON FINAL→FIRST MATCH =====================\n",
    "G = nx.Graph()\n",
    "\n",
    "# Each MergeID_2 is a node\n",
    "for mid in df[\"MergeID_2\"]:\n",
    "    G.add_node(mid)\n",
    "\n",
    "for idx_a, row_a in df.iterrows():\n",
    "    for idx_b, row_b in df.iterrows():\n",
    "        if row_a.MergeID_2 == row_b.MergeID_2:\n",
    "            continue\n",
    "\n",
    "        dx = abs(row_a.LastX - row_b.FirstX)\n",
    "        dy = abs(row_a.LastY - row_b.FirstY)\n",
    "        frame_diff = abs(row_a.LastFrame - row_b.FirstFrame)\n",
    "\n",
    "        # Merge if within thresholds\n",
    "        if dx <= X_THRESH and dy <= Y_THRESH and frame_diff <= FRAME_THRESH:\n",
    "            G.add_edge(row_a.MergeID_2, row_b.MergeID_2)\n",
    "\n",
    "# ===================== ASSIGN NEW MERGEIDs =====================\n",
    "components = list(nx.connected_components(G))\n",
    "merge_map = {}\n",
    "for new_merge_id, comp in enumerate(components, start=1):\n",
    "    for old_merge_id in comp:\n",
    "        merge_map[old_merge_id] = new_merge_id\n",
    "\n",
    "df[\"MergeID_3\"] = df[\"MergeID_2\"].map(merge_map).fillna(-1).astype(int)\n",
    "\n",
    "# ===================== SAVE THIRD PASS =====================\n",
    "df.to_csv(OUTPUT_MERGED, index=False)\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "summary_rows = []\n",
    "for merge_id in sorted(df[\"MergeID_3\"].unique()):\n",
    "    subset = df[df[\"MergeID_3\"] == merge_id].sort_values(by=\"FirstFrame\")\n",
    "    first_row = subset.iloc[0]\n",
    "    last_row = subset.iloc[-1]\n",
    "\n",
    "    # Flatten PersonIDs safely\n",
    "    merged_persons = []\n",
    "    for p in subset[\"PersonIDs\"]:\n",
    "        if isinstance(p, str):\n",
    "            merged_persons.extend(ast.literal_eval(p))\n",
    "        else:\n",
    "            merged_persons.append(p)\n",
    "    merged_persons = sorted(set(merged_persons))\n",
    "\n",
    "    # Preserve earliest ArrivalZone from previous-pass groups\n",
    "    arrival_subset = subset.dropna(subset=[\"ArrivalTime\", \"ArrivalZone\"])\n",
    "    if not arrival_subset.empty:\n",
    "        # Earliest arrival\n",
    "        arrival_row = arrival_subset.loc[arrival_subset[\"ArrivalTime\"].idxmin()]\n",
    "        arrival_time = arrival_row[\"ArrivalTime\"]\n",
    "        arrival_zone = arrival_row[\"ArrivalZone\"]\n",
    "\n",
    "        # DepartureTime: last time in the same arrival_zone across all merged groups\n",
    "        same_zone_rows = subset[subset[\"ArrivalZone\"] == arrival_zone]\n",
    "        if not same_zone_rows.empty:\n",
    "            departure_time = same_zone_rows[\"DepartureTime\"].max()\n",
    "        else:\n",
    "            departure_time = arrival_time\n",
    "    else:\n",
    "        arrival_time, arrival_zone, departure_time = None, None, None\n",
    "\n",
    "    # LastTime / LastZone: last detection overall among merged rows\n",
    "    last_detection_rows = subset.dropna(subset=[\"LastTime\", \"LastZone\"])\n",
    "    if not last_detection_rows.empty:\n",
    "        last_row_final = last_detection_rows.iloc[-1]\n",
    "        last_time = last_row_final[\"LastTime\"]\n",
    "        last_zone = last_row_final[\"LastZone\"]\n",
    "    else:\n",
    "        last_time, last_zone = None, None\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"MergeID_3\": merge_id,\n",
    "        \"PersonIDs\": merged_persons,\n",
    "        \"FirstTime\": first_row[\"FirstTime\"],\n",
    "        \"FirstFrame\": first_row[\"FirstFrame\"],\n",
    "        \"FirstX\": first_row[\"FirstX\"],\n",
    "        \"FirstY\": first_row[\"FirstY\"],\n",
    "        \"FirstZone\": first_row.get(\"FirstZone\", \"\"),\n",
    "        \"ArrivalTime\": arrival_time,\n",
    "        \"ArrivalZone\": arrival_zone,\n",
    "        \"DepartureTime\": departure_time,\n",
    "        \"LastTime\": last_time,\n",
    "        \"LastFrame\": last_row[\"LastFrame\"],\n",
    "        \"LastX\": last_row[\"LastX\"],\n",
    "        \"LastY\": last_row[\"LastY\"],\n",
    "        \"LastZone\": last_zone\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(OUTPUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"Saved third-pass merged dataset → {OUTPUT_MERGED}\")\n",
    "print(f\"Saved third-pass summary → {OUTPUT_SUMMARY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f366c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned table saved to /Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2-summary3-cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "#final clean up - delete row if time is 1s or less\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "INPUT_FILE = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2-summary3.csv\"\n",
    "OUTPUT_FILE = \"/Users/cherrychoy/Library/CloudStorage/OneDrive-TheUniversityofSydney(Students)/Honours Thesis/Videos/TRIAL/trial13-postprocess-summary1-summary2-summary3-cleaned.csv\" \n",
    "\n",
    "# ===================== LOAD =====================\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# ===================== CONVERT TIMES =====================\n",
    "# Convert to timedelta for comparison\n",
    "df[\"FirstTime_td\"] = pd.to_timedelta(df[\"FirstTime\"])\n",
    "df[\"LastTime_td\"] = pd.to_timedelta(df[\"LastTime\"])\n",
    "\n",
    "# ===================== FILTER ROWS =====================\n",
    "# Keep only rows where duration > 0 seconds\n",
    "df_clean = df[df[\"LastTime_td\"] != df[\"FirstTime_td\"]].copy()\n",
    "\n",
    "# ===================== REMOVE TEMP COLUMNS =====================\n",
    "df_clean = df_clean.drop(columns=[\"FirstTime_td\", \"LastTime_td\"], errors=\"ignore\")\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "df_clean.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Cleaned table saved to {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_bytetrack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
